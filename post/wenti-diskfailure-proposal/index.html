<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="utf-8" />

  
  <title>今年参加PAKDD2020：阿里巴巴算法写的一个论文: A Voting-based Robust Model for Disk Failure Prediction</title>

  
  
  <link href="//cdn.jsdelivr.net" rel="dns-prefetch">
  <link href="//cdnjs.cloudflare.com" rel="dns-prefetch">
  
  <link href="//at.alicdn.com" rel="dns-prefetch">
  
  <link href="//fonts.googleapis.com" rel="dns-prefetch">
  <link href="//fonts.gstatic.com" rel="dns-prefetch">
  <link href="///disqus.com" rel="dns-prefetch">
  <link href="//c.disquscdn.com" rel="dns-prefetch">
  
  <link href="//www.google-analytics.com" rel="dns-prefetch">
  

  

  
  <meta name="author" content="wentixiaogege">
  <meta name="description" content="">

  
  
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@wentixiaogege">
    <meta name="twitter:title" content="今年参加PAKDD2020：阿里巴巴算法写的一个论文: A Voting-based Robust Model for Disk Failure Prediction">
    <meta name="twitter:description" content="">
    <meta name="twitter:image" content="/images/avatar.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:title" content="今年参加PAKDD2020：阿里巴巴算法写的一个论文: A Voting-based Robust Model for Disk Failure Prediction">
  <meta property="og:description" content="">
  <meta property="og:url" content="/post/wenti-diskfailure-proposal/">
  <meta property="og:image" content="/images/avatar.png">




<meta name="generator" content="Hugo 0.76.3">


<link rel="canonical" href="/post/wenti-diskfailure-proposal/">

<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="format-detection" content="telephone=no,email=no,adress=no">
<meta http-equiv="Cache-Control" content="no-transform">


<meta name="robots" content="index,follow">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="google-site-verification" content="_moDmnnBNVLBN1rzNxyGUGdPHE20YgbmrtzLIbxaWFc">
<meta name="msvalidate.01" content="22596E34341DD1D17D6022C44647E587">





<meta name="theme-color" content="#02b875">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="apple-mobile-web-app-title" content="wentixiaogege">
<meta name="msapplication-tooltip" content="wentixiaogege">
<meta name='msapplication-navbutton-color' content="#02b875">
<meta name="msapplication-TileColor" content="#02b875">
<meta name="msapplication-TileImage" content="/icons/icon-144x144.png">
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/icons/icon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/icons/icon-32x32.png">
<link rel="icon" sizes="192x192" href="/icons/icon-192x192.png">
<link rel="apple-touch-icon" href="/icons/icon-152x152.png">
<link rel="manifest" href="/manifest.json">


<link rel="preload" href="/styles/main-rendered.min.css" as="style">


<link rel="preload" href="https://fonts.googleapis.com/css?family=Lobster" as="style">
<link rel="preload" href="/images/avatar.png" as="image">
<link rel="preload" href="/images/grey-prism.svg" as="image">


<style>
  body {
    background: rgb(244, 243, 241) url('/images/grey-prism.svg') repeat fixed;
  }
</style>
<link rel="stylesheet" href="/styles/main-rendered.min.css">


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lobster">



<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.2/dist/medium-zoom.min.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/video.js@7.3.0/dist/video-js.min.css">



  
  
<!--[if lte IE 8]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/videojs-ie8@1.1.2/dist/videojs-ie8.min.js"></script>
<![endif]-->

<!--[if lte IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/eligrey-classlist-js-polyfill@1.2.20180112/classList.min.js"></script>
<![endif]-->


</head>
  <body>
    <div class="suspension">
      <a role="button" aria-label="Go to top" title="Go to top" class="to-top is-hide"><span class="icon icon-up" aria-hidden="true"></span></a>
      
        
	<a role="button" aria-label="Go to comments" title="Go to comments" class="to-comment" href="#disqus_thread"><span class="icon icon-comment" aria-hidden="true"></span></a>
        
      
    </div>
    
    
  <header class="site-header">
  <a href=""><img class="avatar" src="/images/avatar.png" alt="Avatar"></a>
  
  <h2 class="title"><a href="">wentixiaogege</a></h2>
  
  <p class="subtitle">一半忧与愁 &amp; 一半花与海</p>
  <button class="menu-toggle" type="button" aria-label="Main Menu" aria-expanded="false" tab-index="0">
    <span class="icon icon-menu" aria-hidden="true"></span>
  </button>

  <nav class="site-menu collapsed">
    <h2 class="offscreen">Main Menu</h2>
    <ul class="menu-list">
      
      
      
      
        <li class="menu-item
          
          
           is-active">
          <a href="/">Home</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/reco/">Recommend</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/times/">Time Series</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="https://github.com/wentixiaogege">Github</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="https://www.kaggle.com/wentixiaogege">Kaggler</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/links/">Links</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/tags/">Tags</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/resume/">Resume</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/links/">Links</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/about/">About</a>
        </li>
      
    </ul>
  </nav>
  <nav class="social-menu collapsed">
    <h2 class="offscreen">Social Networks</h2>
    <ul class="social-list"><li class="social-item">
          <a href="mailto:wentixiaogege@163.com" title="Email" aria-label="Email">
            <span class="icon icon-email" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//github.com/wentixiaogege" rel="me" title="GitHub" aria-label="GitHub">
	    <span class="icon icon-github" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//twitter.com/wentixiaogege" rel="me" title="Twitter" aria-label="Twitter">
            <span class="icon icon-twitter" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//weibo.com/wentixiaogege" rel="me" title="Weibo" aria-label="Weibo">
            <span class="icon icon-weibo" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="/images/qrcode.jpg" rel="me" title="Wechat" aria-label="Wechat">
            <span class="icon icon-wechat" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//www.linkedin.com/in/wentixiaogege" rel="me" title="LinkedIn" aria-label="LinkedIn">
            <span class="icon icon-linkedin" aria-hidden="true"></span>
          </a>
        </li></ul>
  </nav>
</header>

  <section class="main post-detail">
    <header class="post-header">
      <h1 class="post-title">今年参加PAKDD2020：阿里巴巴算法写的一个论文: A Voting-based Robust Model for Disk Failure Prediction</h1>
      <p class="post-meta">@wentixiaogege · Oct 9, 2020 · 20 min read</p>
    </header>
    <article class="post-content"><p><strong>A</strong> <strong>Voting-based</strong> <strong>Robust</strong> <strong>Model</strong> <strong>for</strong> <strong>Disk</strong> <strong>Failure</strong></p>
<p><strong>Prediction</strong></p>
<p><strong>Abstract.</strong> The hard drive failure prediction is a vital part of operating and maintainance issues. With the fast growth of the data-driven arti ﬁcial intelligence algorithms, more and more recent researches focus on its application on the current topic. Its eﬀectiveness and powerfulness can be observed through a large number of data experiments. Nevertheless, the prediction accuracy is still a challenging task for dealing with extreme imbalance samples, particularly in big data cases. Rather than merely applying one well-de ﬁned LGB model, this study develops a novel ensemble learning strategy, i.e. a voting-based model, for imporving the prediction accuracy and the reliance. The experiment results show a progress in scores by employing this voting-based model in comparison to the single LGB model. Additionally, a new type of feature, namely the day distance to important dates, was proven to be eﬃcient for improving overall accuracy.</p>
<p><strong>Keywords:</strong> Voting-based Strategy <em>·</em> SMART <em>·</em> LGB model <em>·</em> Hard Drive Failure Prediction.</p>
<p><strong>1 Introduction</strong></p>
<p>With the fast development of modern cloud datacenters, the number of the hard disk drives deployed has grown dramatically, alongside with the absolute number of disk failures. Since these failures have unneglectable inﬂuences on the cloud service quality, the demands of the disk failure detection is increasing as well. Traditional methods mainly follow the rule-based logic by employing SMART (Self-Monitoring, Analysis and Reporting Technology) logs while re- cent researches show that artiﬁcial intelligence algorithm can be a competitive tool to enhance the prediction accuracy and hence gradually becomes a major solution in reality projects. Thereby, for the aim of improving application of AI Supported by Alibaba and Shanghai East Low Carbon Technology  2 M. Li et al. algorithms, this study builds up a well-deﬁned LGB model and subsequently attempts to develop a voting-based strategy. The experiment data source of this project is from 2020 Alibaba AI Ops Competition on Tianchi Platform, i.e. from [1].</p>
<p>For this speciﬁc project, comparing to the SMART data collected from other previous applications, it faces to following challenges during the modeling pro- cess:</p>
<p>a.Extensive data;</p>
<p>b.Missing records on a daily basis, probably due to hardware or network issues; c.Diﬃcult to capture the failure status before the failure occurs;</p>
<p>d.Extreme imbalance samples between the healthy and fault disks;</p>
<p>e.Eﬃcient feature construction.</p>
<p>Multiple recent studies have attempted to address aforementioned problems. To facilitate and simplify computation and modeling, extensive data can be split- ted into several segments of time series and the most relevant time segment is then chosen for the modeling. Missing records problems are widely distributed in the projects and a natural possible way is to apply ﬁlling techniques, e.g. forward and backward ﬁlling, liner and nonlinear interpolation methods, etc. Former studies have found that cubic spline interpolation ensures a ”smooth” change and achieves a higher TPR (True positive rate) comparing to the spline ﬁlling and other methods [2]. Notice that, not many researches speciﬁcally emphasized the solution of an imbalanced dataset. For the extreme imbalance cases, naive upsampling and downsampling are potential source of over-ﬁtting [3]. [3] uti- lized SMOTE (Synthetic Minority Oversampling Technique) for oversampling, yet the precision and recall of the model were decreased.</p>
<p>Except from straightforwardly utilizing given SMART attributes, new fea- tures construction greatly inﬂuences the prediction accuracy. As a time series problem, statistical sliding window features can be generated to illustrate the distribution of SMART attributes. [4] applied a gradient-based strategy to mea- sure value transitions before disk failures, and its eﬃciency is validated through the data experiments. A feature combination idea was also brought up in [4], i.e. to take diﬀerent fault types into account. Nevertheless, it was not clearly presented in [4] how original SMART attributes were combined with new fea- tures. Counting the number of attributes that are above zero is another potential approach to combined features [5]. Data from Backblaze shows that when there are more attributes that are above zero (in a certain group), it is more likely to indicate a disk failure.</p>
<p>In general, the mainly contributions of this paper can be concluded as fol- lows: 1) strongly correlated features are extracted based on the data analysis and experiments, i.e. distance to important date, disk usage life, time series slope fea- tures and division features; 2) the correlation analysis, which relies on Pearson</p>
<p>A Voting-based Robust Model for Disk Failure Prediction 3</p>
<p>and Spearman correlation coeﬃcient, is employed for the feature selection, and latter is proved to be more eﬀective in this speciﬁc case; 3) a voting-based strat- egy is developed to ensemble several LGB models with diﬀerent hyperparameters to imporve the accuracy of the disk failure prediction.</p>
<p><strong>2 Feature Engineering</strong></p>
<p>As the basis of a robust prediction model, feature engineering has its irreplace- able place throughout every machine learning process. In this section, various feature processing methods will be illustrated, and their impact on predicting the result will be further discussed in section 4.</p>
<p><strong>2.1</strong> <strong>Data</strong> <strong>Analysis</strong></p>
<p>Dataset provided by 2020 Alibaba AI Ops Competition includes daily SMART logs ranging from July 2017 to July 2018, where all disks belong to a single manufacturer “A”but with diﬀerent models (model 1 and model 2). The goal of the competition was to predict failure time of disks, and results are evalu- ated by F1 scores in next 30 days. Thus, for the aim of simpliﬁcation, the disk would fail or not in next 30 days, denoted as 0 and 1, respectively. Directly after- wards, data exploration was conducted to gain an overall impression on the data.</p>
<p>First of all, Fig. 1 demonstrates failed samples only occupies round 0.08% within the entire train set. It indicates the fact that the train data fed into the model are extremly imbalanced. Undersampling approach, i.e. bagging, was tested on Alibaba’s dataset with some modiﬁcation. Diﬀerent from other imbal- anced problems such as ﬁnancial fraud where samples are mostly independent from each other, predicting disk failures is more diﬃcult because each model contains continuous data points. To prevent information leak by simply doing bagging on SMART daily logs, bagging on disk serial numbers was applied.</p>
<p>Second, not all original features given are useful for this task. 510 original features were provided by the organizer, i.e. from smart 1 to smart 255. Each attribute possesses both a raw and a normalized value, where the former is mea- sured by the sensors, and relied on the former, the latter is normalized by the manufacturer. For the case with a large number of attributes, attributes selec- tion is full of challeges and arts. Consider the usefulness and completeness of the SMART attributes, we remove features that contains only Nan values and that does not change for all training and testing data samples. Therefore, the dimension of the original attributes is truncated from 510 to 45.</p>
<p>Third, some SMART attribute pairs are highly correlated. As mentioned, normalized attributes are generated by the corresponding raw attributes. It can be inferred that, there could be a strong linear correlation between each pair.4 M. Li et al.</p>
<table>
<thead>
<tr>
<th></th>
<th style="text-align:center"></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td style="text-align:center"><img src="" alt="img"></td>
</tr>
</tbody>
</table>
<p><strong>Fig.</strong> <strong>1.</strong> Samples Imbalance Situation of the Dataset</p>
<p>Furthermore, some SMART attributes share similar physical meanings and could be non-linearly correlated. This inﬂuences might further reach to model train- ing, sometimes negative to some extend, and could lead to potential over-ﬁtting problem. Since they could provide duplicate information. From this perspective, the original feature collection should be restrained by removing strong-correlated features. During the testing, both Pearson and Spearman methods are applied to evaluate the correlation between SMART attributes and labels.</p>
<p>Fourth, the raw SMART attributes are skewed and needed to be properly transformed. Most machine learning methods are based on the gradient descent algorithm and as known that their performances can be signiﬁcantly inﬂuenced by the given data distribution. Previous data experiments show a distribution, which is close to the Gaussian distribution, usually can provide high stability and good accuracy in comparison to non-Gaussian ditributions. To this end, the log-normal transformation was employed for these raw attributes:</p>
<p><em>f</em> = <em>log</em> (<em>f</em> + 1) (1)</p>
<p>in which <em>f</em> is the feature after log-normal transformation; <em>f</em> is the original feature.</p>
<p><strong>2.2</strong> <strong>Feature</strong> <strong>Generation</strong> <strong>and</strong> <strong>Selection</strong></p>
<p><strong>Distance</strong> <strong>to</strong> <strong>Important</strong> <strong>Dates.</strong> During the data exploration phase, the distri- bution of the failed disks was pictured to investigate the trend of disk failures. It was found that more disks would fail on days when important activities were closeby. Understanding the application of the hard disks can shed some light on the feature generation. By summarizing the number of failed disks on certain days (shown in Fig. 2), following conclusions could be drawn: (a) disks were more likely to fail just 1 or 2 days before the starts of the next month (or the ends of the current month); (b) disks were more likely to fail at a few days before impor- tant holidays; (c) some peaks can be found after certain holidays. The highest peak observed occurred on Jan 23, 2018, which was about two weeks before the Chinese Lunar New Year. These trends brought a thought that a great portion of the disks were utilized for railway ticket reservation or accounting-related ap- plication.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><img src="" alt="img"><img src="" alt="img"><img src="" alt="img"><img src="" alt="img"><img src="" alt="img"></td>
</tr>
</tbody>
</table>
<p><strong>Fig.</strong> <strong>2.</strong> Trend of failed disks</p>
<p>In Fig. 2, Holiday 1-4 refers to Chinese National Day, Chinese Lunar New Year’s Day, QingMing, and Labor’s day separately. Among those holidays, the National Day is the longest holiday (7 days). The trend around the National Day shows that peaks appeared before and after the holiday, while maintaining a low level in between. This indicates the fact that whether the day is holiday or not does not matter in this task, but the location relative to the holiday matters. Here a set of new features were proposed: days to next important date, and days to last important date. Important dates were deﬁned based on Chinese holidays and the start of the month in this study. It is worth noting here that researchers should gain some understanding of what disks are used for, since diﬀerent indus- tries and countries have their own important dates. For instance, it is reasonable for a ticket booking system to observe increased disk failures before holidays but this is not reasonable for a manufacturer quality system due to their diﬀerent business patterns.</p>
<p><strong>Disk</strong> <strong>Usage</strong> <strong>Life.</strong> With the disk’s usage life increases, the probability of the disk failure becomes higher. Common life span of a hard disk could be in a range of 3-5 years. Although the accumulated training data of Alibaba’s disks only last around 1 year and haven’t reached normal life end, the disk usage life could still be likely to provide some useful information in failure prediction.</p>
<p><strong>Combined</strong> <strong>SMART</strong> <strong>feature.</strong> As discussed in the Introduction section, multiple SMART attributes reaching above zero might indicate a potential disk failure. To validate whether the same trend exists in Alibaba’s dataset, the whole data set, i.e. dating from June 2018, was used for exploration. In this study, fol- lowing group of SMART attributes were selected based on their physical impor- tance: smart 5raw, smart 187raw, smart 188raw, smart 189raw, smart 197raw, and smart 198raw. The result is displayed below in Figure 6, where x-axis refers 6 M. Li et al.to the number of attributes that are above zero. When none of the selected six attributes reaches above zero, 99.95% of those samples are healthy. As the number of larger-than-zero attributes grows, the possibility of failure also rises. Fig. 3 indicates a similar trend described in [5], thus can be a potential strong feature in this problem.</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><img src="" alt="img"></td>
</tr>
<tr>
<td></td>
<td><strong>Fig.</strong> <strong>3.</strong> Trend of sample distribution using combined SMART feature</td>
</tr>
<tr>
<td></td>
<td><img src="" alt="img"><img src="" alt="img"><img src="" alt="img"></td>
</tr>
</tbody>
</table>
<p><strong>Fig.</strong> <strong>4.</strong> Smart 189raw change before disk failed</p>
<p><strong>Time</strong> <strong>Series</strong> <strong>Slope Features.</strong> According to the physical meaning of each SMART attribute [6], many attributes are not real-time but accumulated through- out the running time, such as smart 9 (represents for Power-On Hours). For those attributes, the speciﬁc value gives less information than the value changes. Be- sides, even for attributes that indicate real-time situation, the value changes along the time series could also indicate a status change which relates to disk health. Here one example is shown in Fig. 4: disk 20073 of model 1. The fail date for the disk is June 10, 2018, so based on the label strategy described in section 2.1, all samples after May 11, 2018 (shown as the red dashed line) is labeled as 1. The y axis is the raw smart 189 after the log-normal transformation [see Eq.(1)]. The two steps up after May 11, 2018 could be a signal that the disk was close to its life end. Here in this study, the labeling period was kept as 30 days because of the competition requirement. However, this selection may not apply for all disks.</p>
<p><strong>Division</strong> <strong>Features.</strong> There was another Huawei competition about disk fail- ure prediction which took place the same time as Alibaba’s AI Ops Competition, and the champion solution [7] was published online. It was noticed that a new type of feature was showed a pretty signiﬁcant impact. Here these features would be also taken into consideration to validate their capability in a diﬀerent disk dataset. The division features are constructed as below:</p>
<p>![image-20201009223517730](/Users/jacklee/Library/Application Support/typora-user-images/image-20201009223517730.png)</p>
<p>in which <em>d</em> is the feature after the division transformation; <em>f**raw</em> is the raw feature; <em>f</em>**normalized is the corresponding raw feature after the normalization.</p>
<p>It was not clear enough about the physical meaning of this type of feature, but it does provide some trending information on how linearly the raw and normalized attributes are.</p>
<p><strong>3 Voting Strategy for the Probalistic Approach</strong></p>
<p><strong>3.1</strong> <strong>Basic</strong> <strong>Model</strong></p>
<p>Consider many of the features involved include Nan values and are not ideally continuous, tree-based models are more likely to provide a robust prediction than other type of models such as SVM. Therefore, Microsoft’s LightGBM was selected for its high training performance and eﬃcient memory usage [8]. Be- cause of the extreme imbalanced rate, it was seen that high learning rate and more iterations could potentially lead to over-ﬁtting to the speciﬁc training data used. Thus, after a few experiments the parameters were kept with low learning rate and less iterations.</p>
<p>Several down-sampling approach were tested but none of them showed bet- ter result. As a result, LightGBM’s embedded setting was used to mitigate the imbalance issue. In LightGBM, the parameter ”is unbalanced”provides an ap- proach to deal the imbalance problem with an adjusted loss function [9]:</p>
<p>![image-20201009223620201](/Users/jacklee/Library/Application Support/typora-user-images/image-20201009223620201.png)</p>
<p>in which denote the modeled conditional probabilisties by <em>P</em>+ := <em>P</em> (<em>y</em> = 1*|x*) and *P**−* := *P* (*y* = 0*|**x*), and deﬁne indicators *I*+ (*x**i*) = 1 if *y**i* = 1, and *I**−* (*x**i*) = 1 if *y*i* = 0, vice versa. *n*+ and *n*−* are the number of postive and negative samples, respectively.</p>
<p>Even though the loss function was adjusted for treating imbalanced prob- lem, it was still a logistic approach, and traditional logistic binary classiﬁcation chooses 0.5 as the threshold to separate negative and positive samples. Never- theless, with extreme imbalanced dataset used in this task, sticking to 0.5 would categorize almost all samples as healthy. Therefore, a self-deﬁned threshold was put on the calculated probability to better ﬁltering the most-likely failed samples.</p>
<p><strong>3.2</strong> <strong>Voting</strong> <strong>Strategy</strong> <strong>Framework</strong></p>
<p>Although single LGB model could provide accurate prediction for a group of disk failures, it can also provide completely diﬀerent result when the combination of training parameters were slighted changed or a diﬀerent month of data was used for training. There is a possibility that not all LGB models are equally good at predicting all samples, and thus some measures are needed to combine multiple models.</p>
<p>Traditional model ensemble approaches are mostly depended on model blend- ing or stacking, but when investigating the predicted probability for disk samples, totally diﬀerent probability distribution can be seen for a same set of validation data. This phenomenon implied that predicted probability for failure could be so diﬀerent that an outlier result can dominate the ﬁnal result in simple blending or stacking operation. Therefore, we propose a more robust voting strategy to minimize the inﬂuences of calculated probability and reliance on a single threshold.</p>
<p><strong>4 Case Study</strong></p>
<p>In order to quantify the eﬀect of features proposed in section 2 and the voting strategy in section 3, multiple cases were tested on Alibaba’s docker platform. The models were tested without knowing the speciﬁc test data, providing a fairly close-to-real-world environment. Furthermore, we limited the number of disks submitted to 140-160, so that results can be compared without the eﬀect of submissions.</p>
<p><strong>4.1</strong> <strong>Attributes</strong> <strong>Filtering</strong></p>
<p>In section 2.1, it was found that many SMART attributes are highly correlated, therefore, ﬁltering out those high-correlated attributes might bring potential beneﬁt to the prediction. Table 2 lists the related tests and the corresponding</p>
<p><strong>Table 1.</strong> Pseudocode for the voting-based robust model.</p>
<p><strong>Algorithm</strong>: The voting-based strategy via a series of LGB models</p>
<p><strong>Step</strong> <strong>1</strong>: Build up a well-de ﬁned LGB model for the binary classiﬁcation.</p>
<p>i) set the objective function as ”binary”;</p>
<p>ii) deﬁne the metric function as ”binary logloss”.</p>
<p><strong>Step</strong> <strong>2</strong>: Select a series of hyperparameters for LGB models.</p>
<p>i) choose a series of values for hyperparameters, i.e. ”learning rate”, ”n estimators” and ”subsample ”;</p>
<p>ii) deﬁne combinations of those parameters;</p>
<p><strong>Step</strong> <strong>3</strong>: Make probability predictions.</p>
<p>For each model, obtain the disks failure probability, i.e.</p>
<p><em>P</em> <em>r</em>i*[*y* = 1*|**x*]*,* *i* = 1*,* 2*, &hellip;, m* in which m is number of models;</p>
<p><strong>Step</strong> <strong>4</strong>: Deﬁne the appropriate threshold.</p>
<p>i) strategy 1: constant threshold <em>θ</em>, e.g. <em>θ</em>=0.005, 0.006, etc.;</p>
<p>ii) strategy 2: adaptive threshold <em>θ</em>, e.g. <em>θ</em>= <em>pencentile</em>(<em>P</em> <em>r</em>i*[*y* = 1*|**x*]*,* 10).;</p>
<p><strong>Step</strong> <strong>5</strong>: Output disk failure time via a voting-based strategy.</p>
<p>if <em>P</em> <em>r</em>i*[*y* = 1*|**x*] *&gt;*= *θ*: count += 1</p>
<p>if count *&gt;*m/2: failure disk;</p>
<p>else: healthy disk.</p>
<p>results.</p>
<p>Before additional features were introduced, high-correlated attributes ﬁltered by</p>
<p><strong>Table</strong> <strong>2.</strong> Result Comparison for Attribute Selection</p>
<table>
<thead>
<tr>
<th>No.</th>
<th>Method Applied</th>
<th>Online Score</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>1a</td>
<td>Single LGB model with non-Nan features</td>
<td>20.57%</td>
<td></td>
</tr>
<tr>
<td>1b</td>
<td>Single LGB model using Pearson correlation to ﬁlter attributes</td>
<td>19.65%</td>
<td></td>
</tr>
<tr>
<td>1c</td>
<td>Single LGB model using Spearman correlation to ﬁlter attributes</td>
<td>22.21%</td>
<td></td>
</tr>
<tr>
<td>2</td>
<td>Compared to 1a, add distance to important dates feature</td>
<td>21.79%</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Compared to 1b, add distance to important dates feature</td>
<td>21.88%</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Compared to 1b, add all generated features and apply voting strategy</td>
<td>22.48%</td>
<td></td>
</tr>
<tr>
<td>13</td>
<td>Compared to 1c, add all generated features and apply voting strategy</td>
<td>21.53%</td>
<td></td>
</tr>
</tbody>
</table>
<p>Pearson correlation slightly reduced the online scores (see 1a and 1b). However, both 1a and 1b indicated the low prediction accuracy and were needed to be im- proved. This circumstance did not change until the distance to important dates were deﬁned and added as features (compare 1a and 2, 1b and 3). Notice that, with ﬁltering out high-correlated attributes by Pearson correlation presented a little better result than with original attributes (see 2 and 3). This unexpected phenonmenon reveals that the features addition might be nonlinear. Compare to Pearson correlation, Spearman correlation is more suitable for evaluating the statistical dependence between the rankings of two variables and has wider ap- plications. Hence, Spearman correlation test 1c was carried out and a signiﬁcant</p>
<table>
<thead>
<tr>
<th></th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td><img src="" alt="img"></td>
</tr>
</tbody>
</table>
<p>improvement can be seen (compare 1b and 1c). Neverthless, the beneﬁt of the Spearman correlation ﬁltering is not always improving the prediction accuracy (see 11 and 13), it can boost the model under certain conditions but not always the case.</p>
<p><strong>4.2</strong> <strong>Features</strong> <strong>addition</strong></p>
<p>In section 2.2, ﬁve types of features were proposed. To better understand the eﬀect of each feature, 7 tests were conducted step by step in Fig. 5.</p>
<p><strong>Fig.</strong> <strong>5.</strong> Result Comparison for Generated Features</p>
<table>
<thead>
<tr>
<th>No.</th>
<th>Method Applied</th>
<th>Online Score</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>1b</td>
<td>Single LGB model with only select low-correlated attributes</td>
<td>19.65%</td>
<td></td>
</tr>
<tr>
<td>3</td>
<td>Compared to 1b, add distance to important dates feature</td>
<td>21.88%</td>
<td></td>
</tr>
<tr>
<td>4</td>
<td>Compared to 3, add hard disk usage life feature</td>
<td>20.94%</td>
<td></td>
</tr>
<tr>
<td>5</td>
<td>Compared to 4, add combined SMART feature</td>
<td>20.08%</td>
<td></td>
</tr>
<tr>
<td>6</td>
<td>Compared to 5, add time series slope features</td>
<td>17.10%</td>
<td></td>
</tr>
<tr>
<td>7</td>
<td>Compared to 5, add corresponding division features</td>
<td>21.68%</td>
<td></td>
</tr>
<tr>
<td>8a</td>
<td>Compared to 5, add time series slope features and corresponding divi- sion features</td>
<td>22.16%</td>
<td></td>
</tr>
</tbody>
</table>
<p>The number of features involved were added one by one from No.1b to No.8. From the result, it was seen that following constructed features did show a signif- icant contribution: distance to important dates, division features. The inﬂuences caused by time series slope features were not clear enough because it lowered the score when comparing No.5 and No.6 but improved the score when comparing No.7 and No.8.</p>
<p><strong>4.3</strong> <strong>Sample</strong> <strong>Imbalance</strong></p>
<p>As explained in section 2.1, the hard disk dataset is extremely imbalanced. Tra- ditional approach for dealing with imbalance problem includes down-sampling and up-sampling, however, with time-series data as the SMART log data, sim- ple up-sampling and down-sampling approach would result in information leak. Therefore, the approach here was to do bagging or duplication on disk serial numbers instead of simply up-sampling or down-sampling daily records. Result Comparison can be found in Table 3.</p>
<p><strong>Table</strong> <strong>3.</strong> Result Comparison for Data Up-sampling and Down-sampling</p>
<table>
<thead>
<tr>
<th>No.</th>
<th>Method Applied</th>
<th>Online Score</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>8a</td>
<td>Single LGB model with all generated features, using May-June 2018 data</td>
<td>22.16%</td>
<td></td>
</tr>
<tr>
<td>8b</td>
<td>Compared to 8, down-sampling negative samples only</td>
<td>17.76%</td>
<td></td>
</tr>
<tr>
<td>8c</td>
<td>Compared to 8, up-sampling positive samples and down-sampling neg- ative samples</td>
<td>16.98%</td>
<td></td>
</tr>
</tbody>
</table>
<p>Due to the memory limits, a full up-sampling test was not able to be pro- cessed. Hence a middle approach was utilized, where down-sampling and up- sampling were combined. It was observed that although these processing method decreased the imbalance extent of the dataset, the model was easily gaining high precision and recall score on the validation set even with low learning rate and less iterations.</p>
<p><strong>4.4</strong> <strong>Training</strong> <strong>Data</strong></p>
<p>Since the imbalance issue unpreventably brought over-ﬁtting, the selection of the training data becomes essential. Three tests were conducted where only training dataset were varied. The results veriﬁes the hypothesis that training data aﬀects the prediction result to a great extent. This phenomenon raises a concern that there may not exist a set of perfect training data that generates the best result for all testing conditions, meaning it would be hard to know which data should be used for training. However, it was still reasonable to utilize more recent data than data long ago.</p>
<p><strong>4.5</strong> <strong>Voting</strong> <strong>Strategy</strong></p>
<p>As described in section 3, single LGB model does not provide robust results, thus using voting strategy does not only improve the accuracy, but also reduce model’s reliance on the probabilistic threshold. In Table 5, it is seen that voting strategy is able to increase the prediction accuracy to some degree, but a bad set of sub-models can also greatly harm the result.</p>
<p><strong>Table</strong> <strong>4.</strong> Result Comparison for Diﬀerent Training Data</p>
<table>
<thead>
<tr>
<th>No.</th>
<th>Method Applied</th>
<th>Online Score</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>8a</td>
<td>Single LGB model with all generated features, using May-June 2018  data</td>
<td>22.16%</td>
<td></td>
</tr>
<tr>
<td>9</td>
<td>Compared to 8, use June-July 2018 as training data</td>
<td>14.53%</td>
<td></td>
</tr>
<tr>
<td>10</td>
<td>Compared to 8, use July 2018 as training data</td>
<td>17.15%</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>Table</strong> <strong>5.</strong> Result Comparison for Voting Strategy</p>
<table>
<thead>
<tr>
<th>No.</th>
<th>Method Applied</th>
<th>Online Score</th>
<th></th>
</tr>
</thead>
<tbody>
<tr>
<td>8a</td>
<td>Single LGB model with all generated features</td>
<td>22.16%</td>
<td></td>
</tr>
<tr>
<td>11</td>
<td>Compared to 8, use voting strategy instead of a single LGB model: train 7 sub-models with May-June 2018 data</td>
<td>22.48%</td>
<td></td>
</tr>
<tr>
<td>12</td>
<td>Compared to 11, add 7 more sub-models trained by June-July 2018 data</td>
<td>17.59%</td>
<td></td>
</tr>
</tbody>
</table>
<p><strong>5 Conclusion</strong></p>
<p>This study investigated various feature construction and ﬁltering methods and proposed a new type of feature, i.e. day distance to important dates. More- over, a developed voting-based strategy algorithm was applied rather than one well-deﬁned LGB model. During the competition, it was seen that day distance features contributed signiﬁcant improvement and the voting-based strategy en- sured a better result. Noted that, although the tests were designed to gradually add optimized sub-approaches, the beneﬁt of these sub-approaches was not seen to be added linearly. It would be of great worth to investigate the impact of feature combination and interaction. Additionally, this study is worthwhile for further analysis on voting-based approach and can be expanded to include other models, e.g. XGBoost, CatBoost etc.</p>
<p><strong>6 Acknowledgements</strong></p>
<p>This study and experiment sources are strongly support by the Shanghai East Low Carbon Technology Industry CO., LTD. and Beijing Megvii Co., Ltd. Thanks to Alibaba, PAKDD for hosting and supporting this competition.</p>
<p><strong>References</strong></p>
<p>\1. <a href="https://github.com/alibaba-edu/dcbrain/tree/master/diskdata">https://github.com/alibaba-edu/dcbrain/tree/master/diskdata</a></p>
<p>\2. Shujie Han, etc. Robust Data Preprocessing for Machine-Learning-Based Disk Fail- ure Prediction in Cloud Production Environments.</p>
<p>\3. Nicolas Aussel, Samuel Jaulin, Guillaume Gandon, Yohan Petetin, Eriza Fazli, et al.. Predictive models of hard drive failures based on operational data. ICMLA 2017 : 16th IEEE International Conference On Machine Learning And Applications, Dec 2017, Cancun, Mexico. pp.619 - 625, ﬀ10.1109/ICMLA.2017.00-92ﬀ. ﬀhal-01703140</p>
<p>\4. Wenjun Yang, etc. Hard Drive Failure Prediction Using Big Data, 2015 IEEE 34th Symposium on Reliable Distributed Systems Workshops</p>
<p>\5. <a href="https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/%5B">https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/[</a> ](<a href="https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/">https://www.backblaze.com/blog/what-smart-stats-indicate-hard-drive-failures/</a>)</p>
<p>\6. <a href="https://en.wikipedia.org/wiki/S.M.A.R.T">https://en.wikipedia.org/wiki/S.M.A.R.T</a>.</p>
<p>\7. <a href="https://mp.weixin.qq.com/s/LEsJvrB4V3YyOAZP-PGLFA">https://mp.weixin.qq.com/s/LEsJvrB4V3YyOAZP-PGLFA</a></p>
<p>\8. LightGBM Repository. <a href="https://github.com/microsoft/LightGBM">https://github.com/microsoft/LightGBM</a></p>
<p>\9. Burges, Christopher JC. ”From ranknet to lambdarank to lambdamart: An overview.”Learning 11.23-581 (2010): 81.</p></article>
    <footer class="post-footer">
      
      <ul class="post-tags">
        
          <li><a href="/tags/taga"><span class="tag">TagA</span></a></li>
        
          <li><a href="/tags/tagb"><span class="tag">TagB</span></a></li>
        
      </ul>
      
      <p class="post-copyright">
        © This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International License，please give source if you wish to quote or reproduce.
      </p>
    </footer>
    
      <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "disqus_shortname" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
    
  </section>
  
<footer class="site-footer">
  <p>© 2017-2020 wentixiaogege</p>
  <p>Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> with theme <a href="https://github.com/laozhu/hugo-nuo" target="_blank" rel="noopener">Nuo</a>.</p>
  
</footer>


<script src="https://cdn.jsdelivr.net/npm/smooth-scroll@15.0.0/dist/smooth-scroll.min.js"></script>



<script async src="https://cdn.jsdelivr.net/npm/video.js@7.3.0/dist/video.min.js"></script>




<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }
    },
  });
</script>
<script type="text/x-mathjax-config">
  // Fix <code> tags after MathJax finishes running. This is a
  // hack to overcome a shortcoming of Markdown. Discussion at
  // https://github.com/mojombo/jekyll/issues/199
  MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
  });
</script>



<script src="/scripts/index.min.js"></script>

<script>
  if ('serviceWorker' in navigator) {
    navigator.serviceWorker.register('\/service-worker.js').then(function() {
      console.log('[ServiceWorker] Registered');
    });
  }
</script>




<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-XXXXXXXX-X', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







  </body>
</html>
