<!DOCTYPE html>
<html lang="en">
<head>

  <meta charset="utf-8" />

  
  <title>åœ¨XGBoostå’ŒLightGBMä¸­è‡ªå®šä¹‰æŸå¤±å‡½æ•°</title>

  
  
  <link href="//cdn.jsdelivr.net" rel="dns-prefetch">
  <link href="//cdnjs.cloudflare.com" rel="dns-prefetch">
  
  <link href="//at.alicdn.com" rel="dns-prefetch">
  
  <link href="//fonts.googleapis.com" rel="dns-prefetch">
  <link href="//fonts.gstatic.com" rel="dns-prefetch">
  <link href="///disqus.com" rel="dns-prefetch">
  <link href="//c.disquscdn.com" rel="dns-prefetch">
  
  <link href="//www.google-analytics.com" rel="dns-prefetch">
  

  

  
  
  <meta name="description" content="åœ¨LightGBMå’ŒXGBoostä¸­çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆCustom Loss Functionï¼‰ã€‚
Training loss and Validation loss  Training loss: This is the function that is optimized on the training data. For example, in a neural network binary classifier, this is usually the binary cross entropy. For the random forest classifier, this is the Gini impurity. The training loss is often called the â€œobjective functionâ€ as well. The training loss in LightGBM is called objective.
Validation loss: This is the function that we use to evaluate the performance of our trained model on unseen data.">

  
  
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@wentixiaogege">
    <meta name="twitter:title" content="åœ¨XGBoostå’ŒLightGBMä¸­è‡ªå®šä¹‰æŸå¤±å‡½æ•°">
    <meta name="twitter:description" content="åœ¨LightGBMå’ŒXGBoostä¸­çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆCustom Loss Functionï¼‰ã€‚
Training loss and Validation loss  Training loss: This is the function that is optimized on the training data. For example, in a neural network binary classifier, this is usually the binary cross entropy. For the random forest classifier, this is the Gini impurity. The training loss is often called the â€œobjective functionâ€ as well. The training loss in LightGBM is called objective.
Validation loss: This is the function that we use to evaluate the performance of our trained model on unseen data.">
    <meta name="twitter:image" content="/images/avatar.png">
  

  
  <meta property="og:type" content="article">
  <meta property="og:title" content="åœ¨XGBoostå’ŒLightGBMä¸­è‡ªå®šä¹‰æŸå¤±å‡½æ•°">
  <meta property="og:description" content="åœ¨LightGBMå’ŒXGBoostä¸­çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆCustom Loss Functionï¼‰ã€‚
Training loss and Validation loss  Training loss: This is the function that is optimized on the training data. For example, in a neural network binary classifier, this is usually the binary cross entropy. For the random forest classifier, this is the Gini impurity. The training loss is often called the â€œobjective functionâ€ as well. The training loss in LightGBM is called objective.
Validation loss: This is the function that we use to evaluate the performance of our trained model on unseen data.">
  <meta property="og:url" content="/post/wenti-custom-loss-lgb-xgb/">
  <meta property="og:image" content="/images/avatar.png">




<meta name="generator" content="Hugo 0.76.3">


<link rel="canonical" href="/post/wenti-custom-loss-lgb-xgb/">

<meta name="renderer" content="webkit">
<meta name="viewport" content="width=device-width,initial-scale=1">
<meta name="format-detection" content="telephone=no,email=no,adress=no">
<meta http-equiv="Cache-Control" content="no-transform">


<meta name="robots" content="index,follow">
<meta name="referrer" content="origin-when-cross-origin">
<meta name="google-site-verification" content="_moDmnnBNVLBN1rzNxyGUGdPHE20YgbmrtzLIbxaWFc">
<meta name="msvalidate.01" content="22596E34341DD1D17D6022C44647E587">





<meta name="theme-color" content="#02b875">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="black">
<meta name="apple-mobile-web-app-title" content="wentixiaogege">
<meta name="msapplication-tooltip" content="wentixiaogege">
<meta name='msapplication-navbutton-color' content="#02b875">
<meta name="msapplication-TileColor" content="#02b875">
<meta name="msapplication-TileImage" content="/icons/icon-144x144.png">
<link rel="icon" href="/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="/icons/icon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="/icons/icon-32x32.png">
<link rel="icon" sizes="192x192" href="/icons/icon-192x192.png">
<link rel="apple-touch-icon" href="/icons/icon-152x152.png">
<link rel="manifest" href="/manifest.json">


<link rel="preload" href="/styles/main-rendered.min.css" as="style">


<link rel="preload" href="https://fonts.googleapis.com/css?family=Lobster" as="style">
<link rel="preload" href="/images/avatar.png" as="image">
<link rel="preload" href="/images/grey-prism.svg" as="image">


<style>
  body {
    background: rgb(244, 243, 241) url('/images/grey-prism.svg') repeat fixed;
  }
</style>
<link rel="stylesheet" href="/styles/main-rendered.min.css">


<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lobster">



<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.2/dist/medium-zoom.min.js"></script>




<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/video.js@7.3.0/dist/video-js.min.css">



  
  
<!--[if lte IE 8]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/videojs-ie8@1.1.2/dist/videojs-ie8.min.js"></script>
<![endif]-->

<!--[if lte IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/eligrey-classlist-js-polyfill@1.2.20180112/classList.min.js"></script>
<![endif]-->


</head>
  <body>
    <div class="suspension">
      <a role="button" aria-label="Go to top" title="Go to top" class="to-top is-hide"><span class="icon icon-up" aria-hidden="true"></span></a>
      
        
	<a role="button" aria-label="Go to comments" title="Go to comments" class="to-comment" href="#disqus_thread"><span class="icon icon-comment" aria-hidden="true"></span></a>
        
      
    </div>
    
    
  <header class="site-header">
  <a href=""><img class="avatar" src="/images/avatar.png" alt="Avatar"></a>
  
  <h2 class="title"><a href="">wentixiaogege</a></h2>
  
  <p class="subtitle">ä¸€åŠå¿§ä¸æ„ &amp; ä¸€åŠèŠ±ä¸æµ·</p>
  <button class="menu-toggle" type="button" aria-label="Main Menu" aria-expanded="false" tab-index="0">
    <span class="icon icon-menu" aria-hidden="true"></span>
  </button>

  <nav class="site-menu collapsed">
    <h2 class="offscreen">Main Menu</h2>
    <ul class="menu-list">
      
      
      
      
        <li class="menu-item
          
          
          ">
          <a href="https://github.com/wentixiaogege">Github</a>
        </li>
      
        <li class="menu-item
          
          
           is-active">
          <a href="/">Home</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="https://www.kaggle.com/wentixiaogege">Kaggler</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/links/">Links</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/reco/">Recommend</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/resume/">Resume</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/tags/">Tags</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/times/">Time Series</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/links/">Links</a>
        </li>
      
        <li class="menu-item
          
          
          ">
          <a href="/about/">About</a>
        </li>
      
    </ul>
  </nav>
  <nav class="social-menu collapsed">
    <h2 class="offscreen">Social Networks</h2>
    <ul class="social-list"><li class="social-item">
          <a href="mailto:wentixiaogege@163.com" title="Email" aria-label="Email">
            <span class="icon icon-email" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//github.com/wentixiaogege" rel="me" title="GitHub" aria-label="GitHub">
	    <span class="icon icon-github" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//twitter.com/wentixiaogege" rel="me" title="Twitter" aria-label="Twitter">
            <span class="icon icon-twitter" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//weibo.com/wentixiaogege" rel="me" title="Weibo" aria-label="Weibo">
            <span class="icon icon-weibo" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="/images/qrcode.jpg" rel="me" title="Wechat" aria-label="Wechat">
            <span class="icon icon-wechat" aria-hidden="true"></span>
          </a>
        </li><li class="social-item">
          <a href="//www.linkedin.com/in/wentixiaogege" rel="me" title="LinkedIn" aria-label="LinkedIn">
            <span class="icon icon-linkedin" aria-hidden="true"></span>
          </a>
        </li></ul>
  </nav>
</header>

  <section class="main post-detail">
    <header class="post-header">
      <h1 class="post-title">åœ¨XGBoostå’ŒLightGBMä¸­è‡ªå®šä¹‰æŸå¤±å‡½æ•°</h1>
      <p class="post-meta">@é—®é¢˜å°å“¥å“¥ Â· Oct 9, 2020 Â· 5 min read</p>
    </header>
    <article class="post-content"><p>åœ¨LightGBMå’ŒXGBoostä¸­çš„è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼ˆCustom Loss Functionï¼‰ã€‚</p>
<h2 id="training-loss-and-validation-loss">Training loss and Validation loss</h2>
<blockquote>
<p><strong>Training loss:</strong> This is the function that is optimized on the training data. For example, in a neural network binary classifier, this is usually the binary cross entropy. For the random forest classifier, this is the Gini impurity. The training loss is often called the â€œobjective functionâ€ as well. The training loss in LightGBM is called <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective"><code>objective</code></a>.</p>
<p><strong>Validation loss:</strong> This is the function that we use to evaluate the performance of our trained model on unseen data. This is often not the same as the training loss. For example, in the case of a classifier, this is often the area under the curve of the receiver operating characteristic (ROC) â€” though this is never directly optimized, because it is not differentiable. This is often called the â€œperformance or evaluation metricâ€. The validation loss is often used to tune hyper-parameters. It is often easier to customize, as it doesnâ€™t have as many functional requirements like the training loss does. The validation loss can be non-convex, non-differentiable, and discontinuous. The validation loss in LightGBM is called <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#metric"><code>metric</code></a>.</p>
</blockquote>
<p>æˆ‘ä»¬å¯ä»¥ç”¨Validation lossåšearly stoppingï¼šå½“è¿­ä»£æ¬¡æ•°ï¼ˆboosting roundsï¼Œæ ‘çš„æ•°é‡ï¼‰å¢åŠ çš„æ—¶å€™ï¼Œlossç»è¿‡early_stopping_roundsä¸å‡å°ï¼Œåˆ™åœæ­¢è®­ç»ƒã€‚</p>
<p>ä½†æ˜¯å¦‚æœValidation loss functionæ˜¯äºŒé˜¶å¯å¯¼çš„ï¼Œåˆ™å¯ä»¥è€ƒè™‘ç›´æ¥ç”¨å…¶ä½œä¸ºTraining lossç›´æ¥ä¼˜åŒ–æ¨¡å‹ã€‚</p>
<blockquote>
<p><strong>Training loss:</strong> Customizing the training loss in LightGBM requires defining a function that takes in two arrays, the targets and their predictions. In turn, the function should return two arrays of the gradient and hessian of each observation. As noted above, we need to use calculus to derive gradient and hessian and then implement it in Python.</p>
<p><strong>Validation loss:</strong> Customizing the validation loss in LightGBM requires defining a function that takes in the same two arrays, but returns three values: a string with name of metric to print, the loss itself, and a boolean about whether higher is better.</p>
</blockquote>
<p><a href="https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py">å®˜æ–¹ä¾‹å­-LGBMä¸­è‡ªå®šä¹‰log likelihood lossï¼š</a></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># self-defined objective function</span>
<span style="color:#75715e"># f(preds: array, train_data: Dataset) -&gt; grad: array, hess: array</span>
<span style="color:#75715e"># log likelihood loss</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">loglikelihood</span>(preds, train_data):
    labels <span style="color:#f92672">=</span> train_data<span style="color:#f92672">.</span>get_label()
    preds <span style="color:#f92672">=</span> <span style="color:#ae81ff">1.</span> <span style="color:#f92672">/</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">+</span> np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>preds))
    grad <span style="color:#f92672">=</span> preds <span style="color:#f92672">-</span> labels
    hess <span style="color:#f92672">=</span> preds <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> preds)
    <span style="color:#66d9ef">return</span> grad, hess

<span style="color:#75715e"># self-defined eval metric</span>
<span style="color:#75715e"># f(preds: array, train_data: Dataset) -&gt; name: string, eval_result: float, is_higher_better: bool</span>
<span style="color:#75715e"># binary error</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">binary_error</span>(preds, train_data):
    labels <span style="color:#f92672">=</span> train_data<span style="color:#f92672">.</span>get_label()
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;error&#39;</span>, np<span style="color:#f92672">.</span>mean(labels <span style="color:#f92672">!=</span> (preds <span style="color:#f92672">&gt;</span> <span style="color:#ae81ff">0.5</span>)), False
</code></pre></div><h2 id="å®ä¾‹">å®ä¾‹</h2>
<h4 id="1è‡ªå®šä¹‰mse">ï¼ˆ1ï¼‰è‡ªå®šä¹‰MSE</h4>
<p>è€ƒè™‘è¿™æ ·ä¸€ç§åœºæ™¯ï¼Œæˆ‘ä»¬èµ¶è½¦èµ¶é£æœºï¼Œé¢„æµ‹æˆ‘ä»¬çš„å‡ºå‘æ—¶é—´ï¼Œä½¿å¾—æˆ‘ä»¬ç­‰å€™æ—¶é—´æœ€å°‘ã€‚å¯¹äºæ—©åˆ°å’Œæ™šåˆ°ï¼Œæƒ©ç½šæ˜¯ä¸ä¸€æ ·çš„ï¼Œæ—©åˆ°æœºåœºç«è½¦ç«™ï¼Œæ— å¯åšéã€‚ä½†æ˜¯è¦æ˜¯æ™šåˆ°ï¼Œå°±éº»çƒ¦äº†â€¦ æ‰€ä»¥æ˜¾è€Œæ˜“è§ï¼Œæˆ‘ä»¬åœ¨å»ºæ¨¡æ—¶å€™éœ€è¦åŠ å¤§è¿Ÿåˆ°çš„æƒ©ç½šã€‚å¦‚ä¸‹customMSEå…¬å¼ï¼Œå¯¹äºè¿Ÿåˆ°æˆ‘ä»¬åŠ å¤§10å€çš„æƒ©ç½šã€‚</p>
<p>ğ‘ğ‘¢ğ‘ ğ‘¡ğ‘œğ‘šğ‘€ğ‘†ğ¸=1ğ‘âˆ‘ğ‘–ğ‘”ğ‘–(ğ‘¥)ğ‘”ğ‘–(ğ‘¥)={ğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–^,ğ‘¦ğ‘–â‰¥ğ‘¦ğ‘–^10Ã—(ğ‘¦ğ‘–âˆ’ğ‘¦ğ‘–^),ğ‘¦ğ‘–&lt;ğ‘¦ğ‘–^customMSE=1Nâˆ‘igi(x)gi(x)={yiâˆ’yi^,yiâ‰¥yi^10Ã—(yiâˆ’yi^),yi&lt;yi^</p>
<p>è¯¥å‡½æ•°åŠå…¶gradientå’Œhessianå¯è§†åŒ–ï¼š</p>
<p><img src="https://www.wentixiaogege.com/post/pics/image-20191010151603301.png" alt="img"></p>
<p><img src="https://www.wentixiaogege.com/post/pics/image-20191010151711499.png" alt="img"></p>
<p>åˆ™å¯ä»¥è‡ªå®šä¹‰training losså’Œvalidation lossï¼š</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">custom_asymmetric_train</span>(y_true, y_pred):
    residual <span style="color:#f92672">=</span> (y_true <span style="color:#f92672">-</span> y_pred)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#34;float&#34;</span>)
    grad <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(residual<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0</span>, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span><span style="color:#ae81ff">10.0</span><span style="color:#f92672">*</span>residual, <span style="color:#f92672">-</span><span style="color:#ae81ff">2</span><span style="color:#f92672">*</span>residual)
    hess <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(residual<span style="color:#f92672">&lt;</span><span style="color:#ae81ff">0</span>, <span style="color:#ae81ff">2</span><span style="color:#f92672">*</span><span style="color:#ae81ff">10.0</span>, <span style="color:#ae81ff">2.0</span>)
    <span style="color:#66d9ef">return</span> grad, hess

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">custom_asymmetric_valid</span>(y_true, y_pred):
    residual <span style="color:#f92672">=</span> (y_true <span style="color:#f92672">-</span> y_pred)<span style="color:#f92672">.</span>astype(<span style="color:#e6db74">&#34;float&#34;</span>)
    loss <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>where(residual <span style="color:#f92672">&lt;</span> <span style="color:#ae81ff">0</span>, (residual<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>)<span style="color:#f92672">*</span><span style="color:#ae81ff">10.0</span>, residual<span style="color:#f92672">**</span><span style="color:#ae81ff">2</span>) 
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#34;custom_asymmetric_eval&#34;</span>, np<span style="color:#f92672">.</span>mean(loss), False
<span style="color:#f92672">import</span> lightgbm
<span style="color:#75715e"># ********* Sklearn API **********</span>
<span style="color:#75715e"># default lightgbm model with sklearn api</span>
gbm <span style="color:#f92672">=</span> lightgbm<span style="color:#f92672">.</span>LGBMRegressor() 
<span style="color:#75715e"># updating objective function to custom</span>
<span style="color:#75715e"># default is &#34;regression&#34;</span>
<span style="color:#75715e"># also adding metrics to check different scores</span>
gbm<span style="color:#f92672">.</span>set_params(<span style="color:#f92672">**</span>{<span style="color:#e6db74">&#39;objective&#39;</span>: custom_asymmetric_train}, metrics <span style="color:#f92672">=</span> [<span style="color:#e6db74">&#34;mse&#34;</span>, <span style="color:#e6db74">&#39;mae&#39;</span>])
<span style="color:#75715e"># fitting model </span>
gbm<span style="color:#f92672">.</span>fit(
    X_train,
    y_train,
    eval_set<span style="color:#f92672">=</span>[(X_valid, y_valid)],
    eval_metric<span style="color:#f92672">=</span>custom_asymmetric_valid,
    verbose<span style="color:#f92672">=</span>False)
y_pred <span style="color:#f92672">=</span> gbm<span style="color:#f92672">.</span>predict(X_valid)

<span style="color:#75715e"># ********* Python API **********</span>
<span style="color:#75715e"># create dataset for lightgbm</span>
<span style="color:#75715e"># if you want to re-use data, remember to set free_raw_data=False</span>
lgb_train <span style="color:#f92672">=</span> lgb<span style="color:#f92672">.</span>Dataset(X_train, y_train, free_raw_data<span style="color:#f92672">=</span>False)
lgb_eval <span style="color:#f92672">=</span> lgb<span style="color:#f92672">.</span>Dataset(X_valid, y_valid, reference<span style="color:#f92672">=</span>lgb_train, free_raw_data<span style="color:#f92672">=</span>False)
<span style="color:#75715e"># specify your configurations as a dict</span>
params <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#39;objective&#39;</span>: <span style="color:#e6db74">&#39;regression&#39;</span>, <span style="color:#e6db74">&#39;verbose&#39;</span>: <span style="color:#ae81ff">0</span>}
gbm <span style="color:#f92672">=</span> lgb<span style="color:#f92672">.</span>train(params,
                lgb_train,
                num_boost_round<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>,
                init_model<span style="color:#f92672">=</span>gbm,
                fobj<span style="color:#f92672">=</span>custom_asymmetric_train,
                feval<span style="color:#f92672">=</span>custom_asymmetric_valid,
                valid_sets<span style="color:#f92672">=</span>lgb_eval)           
y_pred <span style="color:#f92672">=</span> gbm<span style="color:#f92672">.</span>predict(X_valid)
</code></pre></div><p><a href="https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb">å®Œæ•´ä»£ç åœ¨è¿™é‡Œ</a></p>
<h4 id="2cost-sensitive-logloss">ï¼ˆ2ï¼‰Cost-sensitive Logloss</h4>
<p>åŒæ¯”ä¸Šè¿°ä¾‹å­ï¼Œåœ¨ç–¾ç—…è¯Šæ–­ä¸­ï¼ŒFNå’ŒFPçš„æƒ©ç½šä¹Ÿåº”è¯¥æ˜¯è¦ä¸ä¸€æ ·çš„ã€‚æ²¡ç—…åˆ¤æ–­æˆæœ‰ç—…è¿˜å¥½ï¼ˆFP: False Positiveï¼‰ï¼Œæœ‰ç—…åˆ¤æ–­ä¸ºæ²¡ç—…åˆ™ä¼šæ›´å¯æ€•ï¼ˆFN: False Negativeï¼‰ï¼Œæ¯”å¦‚æ¼è¯Šäº†ç™Œç—‡è€½è¯¯äº†æœ€ä½³æ²»ç–—æ—¶é—´ã€‚æ‰€ä»¥æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªLossåŠ å¤§å¯¹FNçš„æƒ©ç½šï¼š</p>
<p>ğ‘ğ‘¢ğ‘ ğ‘¡ğ‘œğ‘šğ¿ğ‘œğ‘”ğ¿ğ‘œğ‘ ğ‘ ğ¹ğ‘ğ¹ğ‘ƒğ‘¦Ì‚ ğ‘=âˆ’1ğ‘âˆ‘ğ‘–(5Ã—ğ¹ğ‘+ğ¹ğ‘ƒ)=ğ‘¦Ã—log(ğ‘¦Ì‚ )=(1âˆ’ğ‘¦)Ã—log(1âˆ’ğ‘¦Ì‚ )=ğ‘šğ‘–ğ‘›(ğ‘šğ‘ğ‘¥(ğ‘,10âˆ’7),1âˆ’10âˆ’7)=11+ğ‘’âˆ’ğ‘¥customLogLoss=âˆ’1Nâˆ‘i(5Ã—FN+FP)FN=yÃ—logâ¡(y^)FP=(1âˆ’y)Ã—logâ¡(1âˆ’y^)y^=min(max(p,10âˆ’7),1âˆ’10âˆ’7)p=11+eâˆ’x</p>
<p>è¯¥Loss Functionçš„gradient å’Œ hessianï¼š</p>
<p>ğ‘‘ğ¿ğ‘œğ‘ ğ‘ ğ‘‘ğ‘¥=4ğ‘ğ‘¦+ğ‘âˆ’5ğ‘¦ğ‘‘2ğ¿ğ‘œğ‘ ğ‘ ğ‘‘ğ‘¥2=(4ğ‘¦+1)âˆ—ğ‘(1.0âˆ’ğ‘)dLossdx=4py+pâˆ’5yd2Lossdx2=(4y+1)âˆ—p(1.0âˆ’p)</p>
<p>å…¶ä¸­sigmoidå‡½æ•°æ±‚å¯¼ï¼š</p>
<p>ğ‘=ğ‘’ğ‘¥ğ‘’ğ‘¥+1, 1âˆ’ğ‘=1ğ‘’ğ‘¥+1ğ‘‘ğ‘ğ‘‘ğ‘¥=ğ‘’ğ‘¥(ğ‘’ğ‘¥+1)âˆ’(ğ‘’ğ‘¥âˆ—ğ‘’ğ‘¥)(ğ‘’ğ‘¥+1)2=ğ‘’ğ‘¥(ğ‘’ğ‘¥+1)2=ğ‘âˆ—(1âˆ’ğ‘)p=exex+1, 1âˆ’p=1ex+1dpdx=ex(ex+1)âˆ’(exâˆ—ex)(ex+1)2=ex(ex+1)2=pâˆ—(1âˆ’p)</p>
<p>Pythonä¸­è‡ªå®šä¹‰training losså’Œvalidation lossï¼š</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">logistic_obj</span>(y_hat, dtrain):
    y <span style="color:#f92672">=</span> dtrain<span style="color:#f92672">.</span>get_label()
    p <span style="color:#f92672">=</span> y_hat 
    <span style="color:#75715e"># p = 1. / (1. + np.exp(-y_hat)) # ç”¨äºé¿å…hessiançŸ©é˜µä¸­å¾ˆå¤š0</span>
    grad <span style="color:#f92672">=</span> p <span style="color:#f92672">-</span> y
    hess <span style="color:#f92672">=</span> p <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.</span> <span style="color:#f92672">-</span> p)
    grad <span style="color:#f92672">=</span> <span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> p <span style="color:#f92672">*</span> y <span style="color:#f92672">+</span> p <span style="color:#f92672">-</span> <span style="color:#ae81ff">5</span> <span style="color:#f92672">*</span> y
    hess <span style="color:#f92672">=</span> (<span style="color:#ae81ff">4</span> <span style="color:#f92672">*</span> y <span style="color:#f92672">+</span> <span style="color:#ae81ff">1</span>) <span style="color:#f92672">*</span> (p <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> p))
    <span style="color:#66d9ef">return</span> grad, hess

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">err_rate</span>(y_hat, dtrain):
    y <span style="color:#f92672">=</span> dtrain<span style="color:#f92672">.</span>get_label()
    <span style="color:#75715e"># y_hat = 1.0 / (1.0 + np.exp(-y_hat)) # ç”¨äºé¿å…hessiançŸ©é˜µä¸­å¾ˆå¤š0</span>
    y_hat <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>clip(y_hat, <span style="color:#ae81ff">10e-7</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">-</span><span style="color:#ae81ff">10e-7</span>)
    loss_fn <span style="color:#f92672">=</span> y<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log(y_hat)
    loss_fp <span style="color:#f92672">=</span> (<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> y)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1.0</span> <span style="color:#f92672">-</span> y_hat)
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;error&#39;</span>, np<span style="color:#f92672">.</span>sum(<span style="color:#f92672">-</span>(<span style="color:#ae81ff">5</span><span style="color:#f92672">*</span>loss_fn<span style="color:#f92672">+</span>loss_fp))<span style="color:#f92672">/</span>len(y), False
</code></pre></div><p><strong>åŒæ ·çš„ï¼Œå¦‚æœè¦FPåŠ å¤§æƒ©ç½šï¼š</strong></p>
<p>å‡è®¾ç¬¬iä¸ªé¢„æµ‹æ ·æœ¬çš„Lossä¸ºLï¼Œp(x)ä¸ºsigmoidå‡½æ•°ï¼Œæˆ‘ä»¬ç”¨ ğ›½(&gt;1)Î²(&gt;1)æ¥è¡¨ç¤ºFPçš„æƒé‡ã€‚</p>
<p>ğ¿=âˆ’ğ‘¦lnğ‘âˆ’ğ›½(1âˆ’ğ‘¦)ln(1âˆ’ğ‘) grad =âˆ‚ğ¿âˆ‚ğ‘¥=âˆ‚ğ¿âˆ‚ğ‘âˆ‚ğ‘âˆ‚ğ‘¥=ğ‘(ğ›½+ğ‘¦âˆ’ğ›½ğ‘¦)âˆ’ğ‘¦ hess =âˆ‚2ğ¿âˆ‚ğ‘¥2=ğ‘(1âˆ’ğ‘)(ğ›½+ğ‘¦âˆ’ğ›½ğ‘¦)L=âˆ’ylnâ¡pâˆ’Î²(1âˆ’y)lnâ¡(1âˆ’p) grad =âˆ‚Lâˆ‚x=âˆ‚Lâˆ‚pâˆ‚pâˆ‚x=p(Î²+yâˆ’Î²y)âˆ’y hess =âˆ‚2Lâˆ‚x2=p(1âˆ’p)(Î²+yâˆ’Î²y)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#66d9ef">def</span> <span style="color:#a6e22e">weighted_logloss</span>(y_hat, dtrain):
    y <span style="color:#f92672">=</span> dtrain<span style="color:#f92672">.</span>get_label()
    p <span style="color:#f92672">=</span> y_hat
    beta <span style="color:#f92672">=</span> <span style="color:#ae81ff">5</span>
    grad <span style="color:#f92672">=</span> p <span style="color:#f92672">*</span> (beta <span style="color:#f92672">+</span> y <span style="color:#f92672">-</span> beta<span style="color:#f92672">*</span>y) <span style="color:#f92672">-</span> y
    hess <span style="color:#f92672">=</span> p <span style="color:#f92672">*</span> (<span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> p) <span style="color:#f92672">*</span> (beta <span style="color:#f92672">+</span> y <span style="color:#f92672">-</span> beta<span style="color:#f92672">*</span>y)
    <span style="color:#66d9ef">return</span> grad, hess
</code></pre></div><h4 id="3jdataæ¯”èµ›ä¸­è‡ªå®šä¹‰loss">ï¼ˆ3ï¼‰JDATAæ¯”èµ›ä¸­è‡ªå®šä¹‰Loss</h4>
<p>18å¹´<a href="https://jdata.jd.com/html/detail.html?id=2">å¦‚æœŸè€Œè‡³-ç”¨æˆ·è´­ä¹°æ—¶é—´é¢„æµ‹</a>æ¯”èµ›è¦æ±‚åˆ©ç”¨è„±æ•åçš„äº¬ä¸œçœŸå®ç”¨æˆ·å†å²è¡Œä¸ºæ•°æ®ï¼Œå»ºç«‹ç®—æ³•æ¨¡å‹ï¼Œé¢„æµ‹çƒ­é”€å“ç±»çš„ç”¨æˆ·è´­ä¹°æ—¶é—´ã€‚å…¶ä¸­çš„è¯„ä»·æŒ‡æ ‡å¦‚ä¸‹ï¼š</p>
<p>ğ‘†2=âˆ‘ğ‘¢âˆˆğ‘ˆğ‘Ÿğ‘“(ğ‘¢)|ğ‘ˆğ‘Ÿ|ğ‘“(ğ‘¢)={0,ğ‘¢âˆ‰ğ‘ˆğ‘Ÿ1010+ğ‘‘2ğ‘¢,ğ‘¢âˆˆğ‘ˆğ‘ŸS2=âˆ‘uâˆˆUrf(u)|Ur|f(u)={0,uâˆ‰Ur1010+du2,uâˆˆUr</p>
<p>å…¶ä¸­ï¼Œğ‘ˆğ‘ŸUrä¸ºç­”æ¡ˆç”¨æˆ·é›†åˆï¼Œğ‘‘ğ‘¢duè¡¨ç¤ºç”¨æˆ·ğ‘¢uçš„é¢„æµ‹æ—¥æœŸä¸çœŸå®æ—¥æœŸä¹‹é—´çš„è·ç¦»ã€‚</p>
<p>å…¶ä¸­ä¸€ä¸ªå‚èµ›å›¢é˜Ÿç”¨äº†è‡ªå®šä¹‰çš„losså»ä¼˜åŒ–æ¨¡å‹ï¼š</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#75715e"># S2 loss</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">my_loss</span>(preds, train_data):
    labels <span style="color:#f92672">=</span> train_data<span style="color:#f92672">.</span>get_label()
    <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;s2_error&#39;</span>, np<span style="color:#f92672">.</span>mean(<span style="color:#ae81ff">10</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">10</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>square(preds<span style="color:#f92672">-</span>labels))), True

<span style="color:#75715e"># S2 ä¸€é˜¶ã€äºŒé˜¶å¯¼</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">my_objective</span>(preds, train_data):
    labels <span style="color:#f92672">=</span> train_data<span style="color:#f92672">.</span>get_label()
    d <span style="color:#f92672">=</span> preds<span style="color:#f92672">-</span>labels
    x <span style="color:#f92672">=</span> (<span style="color:#ae81ff">10.</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>square(d))
    grad <span style="color:#f92672">=</span> <span style="color:#f92672">-</span><span style="color:#ae81ff">20</span><span style="color:#f92672">*</span>d<span style="color:#f92672">/</span>np<span style="color:#f92672">.</span>square(x)
    hess <span style="color:#f92672">=</span> <span style="color:#ae81ff">80</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>square(d)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>power(x,<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>)<span style="color:#f92672">-</span><span style="color:#ae81ff">20</span><span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>power(x,<span style="color:#f92672">-</span><span style="color:#ae81ff">2</span>)
    <span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>grad, <span style="color:#f92672">-</span>hess

<span style="color:#75715e"># è®­ç»ƒ</span>
model_day <span style="color:#f92672">=</span> lightgbm<span style="color:#f92672">.</span>train(cu_params,rtrain_set, num_boost_round<span style="color:#f92672">=</span><span style="color:#ae81ff">20000</span>,
                           valid_sets<span style="color:#f92672">=</span>[rvalidation_set], fobj <span style="color:#f92672">=</span> my_objective,
                           feval <span style="color:#f92672">=</span> my_loss, valid_names<span style="color:#f92672">=</span>[<span style="color:#e6db74">&#39;valid&#39;</span>],
                           early_stopping_rounds<span style="color:#f92672">=</span><span style="color:#ae81ff">150</span>, verbose_eval<span style="color:#f92672">=</span><span style="color:#ae81ff">200</span>)
</code></pre></div><h4 id="4focal-loss">ï¼ˆ4ï¼‰Focal loss</h4>
<p>è®ºæ–‡ï¼šhttps://arxiv.org/pdf/1708.02002.pdf</p>
<p>æ•°å­¦åŸç†ï¼š</p>
<h5 id="a-cross-entropy">a. cross entropy</h5>
<p>CE(ğ‘,ğ‘¦)={âˆ’log(ğ‘)âˆ’log(1âˆ’ğ‘) if ğ‘¦=1 otherwise âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’ğ‘t={ğ‘1âˆ’ğ‘ if ğ‘¦=1 otherwise âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’=&gt;CE(ğ‘,ğ‘¦)=CE(ğ‘t)=âˆ’log(ğ‘t)CEâ¡(p,y)={âˆ’logâ¡(p) if y=1âˆ’logâ¡(1âˆ’p) otherwise âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’pt={p if y=11âˆ’p otherwise âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’=&gt;CE(p,y)=CE(pt)=âˆ’logâ¡(pt)</p>
<p>ä¸Šå¼ä¸­CEä¸ºcross entropyï¼Œpä¸ºé¢„æµ‹yä¸º1çš„æ¦‚ç‡ã€‚</p>
<p>cross entropyå¯¹äºæ ·æœ¬ä¸¥é‡ä¸å‡è¡¡çš„é—®é¢˜ï¼Œå¤§å¤šæ•°çš„æ ·æœ¬å¾ˆæ˜“åˆ†ç±»æ­£ç¡®ï¼Œè™½ç„¶å•ä¸ªæ ·æœ¬çš„losså¾ˆå°ï¼Œä½†æ˜¯åŠ èµ·æ¥å´ä¹Ÿä¼šå¾ˆå¤§ï¼Œè¶…è¿‡å°æ ·æœ¬çš„lossï¼Œä½¿å¾—è¿™äº›å°‘ç±»çš„æ ·æœ¬å¾ˆéš¾è¢«æ­£ç¡®é¢„æµ‹ã€‚</p>
<p>ä¸€ä¸ªå¸¸ç”¨çš„åšæ³•æ˜¯é€šè¿‡å¢åŠ æƒé‡å¯¹å°‘æ ·æœ¬å¢å¤§æƒ©ç½šï¼Œå¦‚ä¸‹å…¬å¼ï¼ˆÎ±-balanced CE lossï¼‰ã€‚ï¼ˆ2ï¼‰ä¸­å…¶å®ç”¨çš„å°±æ˜¯è¿™æ ·çš„æ–¹æ³•ã€‚</p>
<p>CE(ğ‘t)=âˆ’ğ›¼tlog(ğ‘t)âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’ğ›¼t={ğ›¼1âˆ’ğ›¼ if ğ‘¦=1 otherwise CE(pt)=âˆ’Î±tlogâ¡(pt)âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’âˆ’Î±t={Î± if y=11âˆ’Î± otherwise</p>
<p>å…¶ä¸­ï¼Œğ›¼âˆˆ[0,1]Î±âˆˆ[0,1]ï¼Œä¸€èˆ¬æ ¹æ®æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ç¡®å®šï¼Œæˆ–è€…ä½œä¸ºè¶…å‚ç”¨cross-validationç¡®å®šã€‚</p>
<h5 id="b-focal-loss">b. focal loss</h5>
<p>focal losså¯ä»¥è¾ƒå¥½çš„è§£å†³æ ·æœ¬ä¸å‡è¡¡é—®é¢˜ï¼Œå®ƒåœ¨è®¾è®¡ä¸Šå‡å°äº†åˆ†ç±»æ­£ç¡®çš„å¤§ç±»æ ·æœ¬ï¼ˆæ˜“é¢„æµ‹ï¼‰çš„æŸå¤±ï¼Œè€Œè®©æ¨¡å‹å¯¹å°ç±»æ ·æœ¬è¿›è¡Œæ›´å¤§çš„â€œå…³æ³¨â€ã€‚</p>
<p>FL(ğ‘t)=âˆ’(1âˆ’ğ‘t)ğ›¾log(ğ‘t)FL(pt)=âˆ’(1âˆ’pt)Î³logâ¡(pt)</p>
<p>å…¶ä¸­ï¼Œğ›¾â‰¥0Î³â‰¥0. åœ¨ğ›¾=2Î³=2çš„æƒ…å†µä¸‹ï¼Œğ‘ğ‘¡=0.9pt=0.9çš„æ ·æœ¬losså°†æ¯”CEçš„ä½100å€ï¼Œğ‘ğ‘¡â‰ˆ0.968ptâ‰ˆ0.968çš„æ ·æœ¬losså°†æ¯”CEçš„ä½1000å€ï¼Œè¿™ä¼šæé«˜åˆ†ç±»é”™è¯¯æ ·æœ¬çš„é‡è¦æ€§ï¼Œå¯¹å…¶è¿›è¡Œæ›´å¤§çš„â€œå…³æ³¨â€ï¼ˆğ‘ğ‘¡â‰¤0.5ptâ‰¤0.5çš„æ ·æœ¬lossä»…æ¯”CEä½4å€ï¼‰ã€‚ åœ¨å®é™…åº”ç”¨ä¸­ï¼ŒÎ±-balanced Focal lossæ›´å¸¸ç”¨ï¼š</p>
<p>FL(ğ‘t)=âˆ’ğ›¼t(1âˆ’ğ‘t)ğ›¾log(ğ‘t)FL(pt)=âˆ’Î±t(1âˆ’pt)Î³logâ¡(pt)</p>
<p><img src="https://www.wentixiaogege.com/post/pics/image-20191010210049114.png" alt="img"></p>
<p><a href="https://github.com/jrzaurin/LightGBM-with-Focal-Loss">Focal Loss for LightGBM</a>:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-python" data-lang="python"><span style="color:#f92672">from</span> scipy.misc <span style="color:#f92672">import</span> derivative

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">focal_loss_lgb</span>(y_pred, dtrain, alpha, gamma):
  a,g <span style="color:#f92672">=</span> alpha, gamma
  y_true <span style="color:#f92672">=</span> dtrain<span style="color:#f92672">.</span>label
  <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">fl</span>(x,t):
  	p <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>x))
  	<span style="color:#66d9ef">return</span> <span style="color:#f92672">-</span>( a<span style="color:#f92672">*</span>t <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>a)<span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>t) ) <span style="color:#f92672">*</span> (( <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> ( t<span style="color:#f92672">*</span>p <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>t)<span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p)) )<span style="color:#f92672">**</span>g) <span style="color:#f92672">*</span> ( t<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log(p)<span style="color:#f92672">+</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>t)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p) )
  partial_fl <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x: fl(x, y_true)
  grad <span style="color:#f92672">=</span> derivative(partial_fl, y_pred, n<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, dx<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>)
  hess <span style="color:#f92672">=</span> derivative(partial_fl, y_pred, n<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, dx<span style="color:#f92672">=</span><span style="color:#ae81ff">1e-6</span>)
  <span style="color:#66d9ef">return</span> grad, hess

<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">focal_loss_lgb_eval_error</span>(y_pred, dtrain, alpha, gamma):
  a,g <span style="color:#f92672">=</span> alpha, gamma
  y_true <span style="color:#f92672">=</span> dtrain<span style="color:#f92672">.</span>label
  p <span style="color:#f92672">=</span> <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">+</span>np<span style="color:#f92672">.</span>exp(<span style="color:#f92672">-</span>y_pred))
  loss <span style="color:#f92672">=</span> <span style="color:#f92672">-</span>( a<span style="color:#f92672">*</span>y_true <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>a)<span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>y_true) ) <span style="color:#f92672">*</span> (( <span style="color:#ae81ff">1</span> <span style="color:#f92672">-</span> ( y_true<span style="color:#f92672">*</span>p <span style="color:#f92672">+</span> (<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>y_true)<span style="color:#f92672">*</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p)) )<span style="color:#f92672">**</span>g) <span style="color:#f92672">*</span> ( y_true<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log(p)<span style="color:#f92672">+</span>(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>y_true)<span style="color:#f92672">*</span>np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span><span style="color:#f92672">-</span>p) )
  <span style="color:#75715e"># (eval_name, eval_result, is_higher_better)</span>
  <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;focal_loss&#39;</span>, np<span style="color:#f92672">.</span>mean(loss), False

focal_loss <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x,y: focal_loss_lgb(x, y, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)
focal_loss_eval <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x,y: focal_loss_lgb_eval_error(x, y, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)
model <span style="color:#f92672">=</span> lgb<span style="color:#f92672">.</span>train(best, self<span style="color:#f92672">.</span>lgtrain, fobj<span style="color:#f92672">=</span>focal_loss, feval<span style="color:#f92672">=</span>focal_loss_eval)

<span style="color:#75715e"># or with f1 eval</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">focal_loss_lgb_f1_score</span>(preds, lgbDataset):
  preds <span style="color:#f92672">=</span> sigmoid(preds)
  binary_preds <span style="color:#f92672">=</span> [int(p<span style="color:#f92672">&gt;</span><span style="color:#ae81ff">0.5</span>) <span style="color:#66d9ef">for</span> p <span style="color:#f92672">in</span> preds]
  y_true <span style="color:#f92672">=</span> lgbDataset<span style="color:#f92672">.</span>get_label()
  <span style="color:#66d9ef">return</span> <span style="color:#e6db74">&#39;f1&#39;</span>, f1_score(y_true, binary_preds), True

focal_loss <span style="color:#f92672">=</span> <span style="color:#66d9ef">lambda</span> x,y: focal_loss_lgb(x, y, alpha<span style="color:#f92672">=</span><span style="color:#ae81ff">0.25</span>, gamma<span style="color:#f92672">=</span><span style="color:#ae81ff">1.</span>)
cv_result <span style="color:#f92672">=</span> lgb<span style="color:#f92672">.</span>cv(
	params,
	train,
	num_boost_round<span style="color:#f92672">=</span>num_boost_round,
	fobj <span style="color:#f92672">=</span> focal_loss,
	feval <span style="color:#f92672">=</span> focal_loss_lgb_f1_score,
	nfold<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>,
	stratified<span style="color:#f92672">=</span>True,
	early_stopping_rounds<span style="color:#f92672">=</span><span style="color:#ae81ff">20</span>)
</code></pre></div><h2 id="reference">REFERENCE</h2>
<ul>
<li><a href="https://datascience.stackexchange.com/questions/26972/cost-sensitive-logloss-for-xgboost">Cost-sensitive Logloss for XGBoost/LGBM</a></li>
<li><a href="https://github.com/microsoft/LightGBM/issues/1230">æ¥ä¸Š-issueä¸­çš„å†…å®¹</a></li>
<li><a href="https://github.com/microsoft/LightGBM/blob/master/examples/python-guide/advanced_example.py">github-lgbm-ä¾‹å­</a></li>
<li><a href="https://towardsdatascience.com/custom-loss-functions-for-gradient-boosting-f79c1b40466d">towardsdatascience-Custom Loss Functions for Gradient Boosting</a></li>
<li><a href="https://github.com/manifoldai/mf-eng-public/blob/master/notebooks/custom_loss_lightgbm.ipynb">æ¥ä¸Š-notebook</a></li>
<li><a href="https://github.com/Microsoft/LightGBM/issues/1644">multi-class-custom-loss</a></li>
<li><a href="https://towardsdatascience.com/lightgbm-with-the-focal-loss-for-imbalanced-datasets-9836a9ae00ca">LightGBM with the Focal Loss for imbalanced datasets</a></li>
<li><a href="https://github.com/jrzaurin/LightGBM-with-Focal-Loss">github-LightGBM-with-Focal-Loss</a></li>
<li><a href="https://github.com/jhwjhw0123/Imbalance-XGBoost">github-xgboost-focal-loss</a></li>
<li><a href="https://github.com/h2oai/driverlessai-recipes/blob/357dab903b27fbd959997dfc908efdf1e34e13a8/models/custom_loss/lightgbm_with_custom_loss.py">lightgbm_with_custom_loss.py</a></li>
<li><a href="https://github.com/dmlc/xgboost/tree/master/demo">xgboost-custom-loss</a></li>
<li><a href="https://www.kaggle.com/chenglongchen/customized-softkappa-loss-in-xgboost">customized-softkappa-loss-in-xgboost</a></li>
<li><a href="https://xgboost.readthedocs.io/en/latest/tutorials/custom_metric_obj.html">Custom Objective and Evaluation Metric In XGBoost Offical Tutorial-RMSLE</a></li>
</ul>
</article>
    <footer class="post-footer">
      
      <p class="post-copyright">
        Â© This post is licensed under a Creative Commons Attribution-NonCommercial 4.0 International Licenseï¼Œplease give source if you wish to quote or reproduce.
      </p>
    </footer>
    
      <div id="disqus_thread"></div>
<script type="application/javascript">
    var disqus_config = function () {
    
    
    
    };
    (function() {
        if (["localhost", "127.0.0.1"].indexOf(window.location.hostname) != -1) {
            document.getElementById('disqus_thread').innerHTML = 'Disqus comments not available by default when the website is previewed locally.';
            return;
        }
        var d = document, s = d.createElement('script'); s.async = true;
        s.src = '//' + "disqus_shortname" + '.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="https://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
      
    
  </section>
  
<footer class="site-footer">
  <p>Â© 2017-2020 wentixiaogege</p>
  <p>Powered by <a href="https://gohugo.io/" target="_blank" rel="noopener">Hugo</a> with theme <a href="https://github.com/laozhu/hugo-nuo" target="_blank" rel="noopener">Nuo</a>.</p>
  
</footer>


<script src="https://cdn.jsdelivr.net/npm/smooth-scroll@15.0.0/dist/smooth-scroll.min.js"></script>



<script async src="https://cdn.jsdelivr.net/npm/video.js@7.3.0/dist/video.min.js"></script>




<script async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [['$','$'], ['\\(','\\)']],
      displayMath: [['$$','$$'], ['\\[','\\]']],
      processEscapes: true,
      processEnvironments: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
      TeX: { equationNumbers: { autoNumber: "AMS" },
      extensions: ["AMSmath.js", "AMSsymbols.js"] }
    },
  });
</script>
<script type="text/x-mathjax-config">
  // Fix <code> tags after MathJax finishes running. This is a
  // hack to overcome a shortcoming of Markdown. Discussion at
  // https://github.com/mojombo/jekyll/issues/199
  MathJax.Hub.Queue(() => {
    MathJax.Hub.getAllJax().map(v => v.SourceElement().parentNode.className += ' has-jax');
  });
</script>



<script src="/scripts/index.min.js"></script>

<script>
  if ('serviceWorker' in navigator) {
    navigator.serviceWorker.register('\/service-worker.js').then(function() {
      console.log('[ServiceWorker] Registered');
    });
  }
</script>




<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-XXXXXXXX-X', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>







  </body>
</html>
